<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- Hi, Jon Here. Please DELETE the two <script> tags below if you use this HTML, otherwise my analytics will track your page -->
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-7580334-2"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-7580334-2');
  </script>

  <title>Weifeng Chen</title>
  
  <meta name="author" content="Weifeng Chen">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/um_logo.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tr style="padding:0px">
      <td style="padding:0px">
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">          
            <tr>
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Weifeng Chen</name>
                  </p>
                  <p>
                    I work on GenAI at Meta. I did my Ph.D. at the University of <a href="https://umich.edu/">Michigan, Ann Arbor</a> advised by <a href="https://www.cs.princeton.edu/~jiadeng/">Prof. Jia Deng</a> in the <a href="https://pvl.cs.princeton.edu/">Princeton Vision & Learning Lab</a>.
                    <!-- Previously, I received my B. Eng degree from Zhejiang University under the supervision of <a href="http://www.cad.zju.edu.cn/home/gfzhang/">Prof. Guofeng Zhang</a>. I spent the last two years of my Ph.D. at the <a href="https://pvl.cs.princeton.edu/">Princeton Vision & Learning Lab</a>.  -->
                  </p>
                  <!-- <p> 
                    My research interests lie in computer vision and machine learning. I am especially interested in research on inferring shapes, motions, and physics from images and videos.
                  </p> -->
                  <p style="text-align:center">
                    <a href="mailto:wfchen@umich.edu">wfchen@umich.edu</a> &nbsp/&nbsp                    
                    <a href="https://scholar.google.com/citations?user=3miKvUMAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                    <a href="https://github.com/wfchen-umich"> Github </a>
                  </p>
              <td style="padding:2.5%;width:33%;max-width:33%">
                <img style="width:200px" src="images/me.png">
              </td>
            </tr> 
          </table>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">          
            <tr><td><heading>Publications</heading></td></tr>


            <tr>            
              <td width="32%">
                <img src='https://jasonqsy.github.io/resources/affordancellm.png' width=100%>
              </td>
              
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/2401.06341">
                  <papertitle>AffordanceLLM: Grounding Affordance from Vision Language Models</papertitle>
                </a>
                <br>
                Shengyi Qian, <b>Weifeng Chen</b>, Min Bai, Xiong Zhou, Zhuowen Tu, Li Erran Li<br>
                <em>OpenSUN3D Workshop @ Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2024. <br></em>
                [<a href="https://arxiv.org/abs/2401.06341">paper</a>]
                <br>
                <p></p>
                <p>We aim to enhance the generalization capability of affordance grounding to in-the-wild objects that are unseen during training, by developing a new approach AffordanceLLM, that takes the advantage of the rich knowledge from large-scale VLMs.</p>
              </td>
            </tr>


            <tr>            
              <td width="32%">
                <img src='images/humble_teacher.png' width=100%>
              </td>
              
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://www.amazon.science/publications/humble-teachers-teach-better-students-for-semi-supervised-object-detection">
                  <papertitle>Humble Teachers Teach Better Students for Semi-supervised Object Detection</papertitle>
                </a>
                <br>
                Yihe Tang, <b>Weifeng Chen</b>, Yijun Luo, Yuting Zhang<br>
                <em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2021. <br></em>
                [<a href="https://www.amazon.science/publications/humble-teachers-teach-better-students-for-semi-supervised-object-detection">paper</a>]
                <br>
                <p></p>
                <p>We propose a semi-supervised approach for contemporary object detectors following the teacher-student dual model framework.</p>
              </td>
            </tr>


            <tr>            
              <td width="32%">
                <img src='images/learning_to_sit.png' width=100%>
              </td>
              
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://arxiv.org/abs/1908.07423v2">
                  <papertitle>Learning to Sit: Synthesizing Human-Chair Interactions via Hierarchical Control</papertitle>
                </a>
                <br>
                Yu-Wei Chao, Jimei Yang, <b>Weifeng Chen</b>, Jia Deng <br>
                <em>AAAI Conference on Artificial Intelligence (<b>AAAI</b>), 2021. <br></em>
                [<a href="https://arxiv.org/abs/1908.07423v2">paper</a>][<a href="https://www.youtube.com/watch?v=3CeN0OGz2cA&feature=youtu.be">video</a>]
                <br>
                <p></p>
                <p>We propose a hierarchical reinforcement framework to learn high-level interactive tasks.</p>
              </td>
            </tr>



            <tr>            
              <td width="32%">
                <img src='images/OASIS.png' width=100%>
              </td>
              
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="https://pvl.cs.princeton.edu/OASIS/">
                  <papertitle>OASIS: A Large-Scale Dataset for Single Image 3D in the Wild</papertitle>
                </a>
                <br>
                <b>Weifeng Chen</b>, Shengyi Qian, David Fan, Noriyuki Kojima, Max Hamilton, Jia Deng<br>              
                <em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2020.<br> </em>
                [<a href="https://arxiv.org/abs/2007.13215">paper</a>][<a href="http://openaccess.thecvf.com/content_CVPR_2020/supplemental/Chen_OASIS_A_Large-Scale_CVPR_2020_supplemental.pdf">supplementary material</a>][<a href="https://pvl.cs.princeton.edu/OASIS/">project site</a>]
                <br>
                <p></p>
                <p> We present Open Annotations of Single Image Surfaces (OASIS), a dataset consisting of detailed 3D geometry for images in the wild.</p>
              </td>
            </tr>




            <tr>            
              <td width="32%">
                <img src='images/youtube3d.png' width=100%>
              </td>
              
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://www-personal.umich.edu/~wfchen/youtube3d/">
                  <papertitle>Learning Single-Image Depth from Videos using Quality Assessment Networks</papertitle>
                </a>
                <br>
                <b>Weifeng Chen</b>, Shengyi Qian, Jia Deng<br>              
                <em>Conference on Computer Vision and Pattern Recognition (<b>CVPR</b>), 2019.<br> </em>
                [<a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Chen_Learning_Single-Image_Depth_From_Videos_Using_Quality_Assessment_Networks_CVPR_2019_paper.pdf">paper</a>][<a href="http://www-personal.umich.edu/~wfchen/youtube3d/">project site</a>][<a href="https://github.com/princeton-vl/YouTube3D">code</a>]
                <br>
                <p></p>
                <p> We propose a method to automatically generate training data for single-view depth through Structure-from-Motion (SfM) on Internet videos.</p>              
              </td>
            </tr>


            <tr>            
              <td width="32%">
                <img src='images/SNOW.png' width=100%>
              </td>
              
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://www-personal.umich.edu/~wfchen/surface-normals-in-the-wild/">
                  <papertitle>Surface Normals in the Wild</papertitle>
                </a>
                <br>
                <b>Weifeng Chen</b>, Donglai Xiang, Jia Deng<br>
                <em>International Conference on Computer Vision (<b>ICCV</b>), 2017. <br> </em>
                [<a href="https://arxiv.org/abs/1704.02956">paper</a>][<a href="http://www-personal.umich.edu/~wfchen/surface-normals-in-the-wild/">dataset</a>][<a href="https://github.com/umich-vl/surface_normals">code</a>] <br>
                Invited [<a href="surface-normals-in-the-wild/iccv_poster_V2.pdf">poster</a>] at the Bridges to 3D Workshop, CVPR 2018
                <br>
                <p></p>
                <p> We present a new dataset "Surface Normals in the Wild" consisting of images in the wild annotated with surface normals of random points.</p>              
              </td>
            </tr>

            <tr>            
              <td width="32%">
                <img src='images/relative_depth.png' width=100%>
              </td>
              
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="http://www-personal.umich.edu/~wfchen/depth-in-the-wild/">
                  <papertitle>Single-Image Depth Perception in the Wild</papertitle>
                </a>
                <br>
                <b>Weifeng Chen</b>, Zhao Fu, Dawei Yang, Jia Deng<br>
                <em>Neural Information Processing Systems (<b>NeurIPS</b>), 2016.  <br> </em>
                [<a href="https://arxiv.org/abs/1604.03901">paper</a>][<a href="http://www-personal.umich.edu/~wfchen/depth-in-the-wild/">dataset</a>][<a href="https://github.com/umich-vl/relative_depth">code</a>][<a href="https://papers.nips.cc/paper/6489-single-image-depth-perception-in-the-wild-supplemental.zip">supplementary material</a>][<a href="https://papers.nips.cc/paper/6489-single-image-depth-perception-in-the-wild/bibtex">BibTex</a>] <br> 
                <p></p>
                <p> We introduce a new dataset "Depth in the Wild" consisting of images in the wild annotated with relative depth between pairs of random points.</p>              
                <strong>Featured in the <a href="https://resources.wolframcloud.com/NeuralNetRepository/tasktype/Regression/">Wolfram Neural Net Repository</a>. See <a href="http://blog.wolfram.com/2018/06/14/launching-the-wolfram-neural-net-repository/">this article</a> for more details.</strong>
              </td>
            </tr>

            <tr>            
              <td width="32%">
                <img src='images/pano-stitching.jpg' width=100%>
              </td>
              
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="data/pano-tip-print.pdf">
                  <papertitle>Multi-Viewpoint Panorama Construction with Wide-Baseline Images</papertitle>
                </a>
                <br>
                Guofeng Zhang, Yi He, <b>Weifeng Chen</b>, Jiaya Jia and Hujun Bao<br>
                <em>IEEE Transactions on Image Processing (<b>TIP</b>), 2016.  <br></em>
                [<a href="data/pano-tip-print.pdf">paper</a>]
                <br>
                <p></p>
                <p>We design a mesh-based framework for creating panoramas from wide-baseline images.</p>
              </td>
            </tr>

            <tr>            
              <td width="32%">
                <img src='images/3DV2014-icon.png' width=100%>
              </td>
              
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a href="data/3DV2014-paper54.pdf">
                  <papertitle>High-Quality Depth Recovery via Interactive Multi-View Stereo</papertitle>
                </a>
                <br>
                <b>Weifeng Chen</b>, Guofeng Zhang, Xiaojun Xiang, Jiaya Jia and Hujun Bao<br>
                <em>International Conference on 3D Vision (<b>3DV</b>), 2014.  <br></em>
                [<a href="data/3DV2014-paper54.pdf">paper</a>]
                <br>
                <p></p>
                <p>We align CAD models interactively to fix artifacts in MVS output.</p>
              </td>
            </tr>


          </table>   <!--  Publications -->



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
              <tr>
                <td width="100%" valign="middle">
                  <heading>Service</heading>
                  <p>                    
                    <strong>Reviewer</strong>: CVPR, ICCV, ECCV, NeurIPS, ICML, ICLR, SIGGRAPH Asia, ICRA, AAAI, WACV, ACCV
                  </p>
                </td>
              </tr>
          </table>
          
          <p align="right">
              Template Credit: <a href="https://jonbarron.info/">Jon Barron</a>
          </p>

      </table>  <!--  Intro + Publications -->
        


      </td>
    </tr>
  </table>      <!--main text-->
</body>

</html>
