
<HTML>
<HEAD>
<title>Single-Image Depth Perception in the Wild</title>
<link rel="stylesheet" href="../jemdoc.css" type="text/css">
</HEAD>
<BODY>
<center>
<h1>Single-Image Depth Perception in the Wild</h1>
<h3>Weifeng Chen, Zhao Fu, Dawei Yang, Jia Deng</h3>

<h4>
{wfchen,zhaofu,ydawei,jiadeng}@umich.edu
</h4>
<p>

<img src="teaser.png" width=800px>
<p><b>Figure 1</b>: Example images and annotations. Green points are those annotated as closer in depth.</p>

<p>
<br>
<table width="80%">
<tr><td align="left">
<p>

<h2>Abstract</h2>
This paper studies single-image depth perception in the wild, i.e., recovering depth
from a single image taken in unconstrained settings. We introduce a new dataset
“Depth in the Wild” consisting of images in the wild annotated with relative depth
between pairs of random points. We also propose a new algorithm that learns to
estimate metric depth using annotations of relative depth. Compared to the state
of the art, our algorithm is simpler and performs better. Experiments show that
our algorithm, combined with existing RGB-D data and our new relative depth
annotations, significantly improves single-image depth perception in the wild.

<h2>Publication</h2>

    
<table>
  <tbody>
    <tr>
      <td width="50">
        <a href="https://arxiv.org/abs/1604.03901">
        <img src="thumbnail.png" border="0" width="100">
        </a>
      </td>

      <td>  
        <h3><a href=""></a></h3>      
        <p>   
          <b>Single-Image Depth Perception in the Wild</b>,<br>
          Weifeng Chen, Zhao Fu, Dawei Yang, Jia Deng<br>
          <em>Neural Information Processing Systems (NIPS), 2016.  <br> </em>
          [<a href="https://arxiv.org/abs/1604.03901">paper</a>][<a href="https://papers.nips.cc/paper/6489-single-image-depth-perception-in-the-wild-supplemental.zip">supplementary material</a>][<a href="https://papers.nips.cc/paper/6489-single-image-depth-perception-in-the-wild/bibtex">BibTex</a>]
        </p>
      </td>     
    </tr>     
  </tbody>
</table>




<tr><td align="left">

<tr><td align="left">



<h2>Dataset</h2>
Download [<a href="https://pvl-weifengc.cs.princeton.edu/DIW/DIW_Annotations.tar.gz">Annotations</a>][<a href="https://pvl-weifengc.cs.princeton.edu/DIW/DIW_test.tar.gz">Test Images (9 GB)</a>][<a href="https://pvl-weifengc.cs.princeton.edu/DIW/DIW_train_val.tar.gz">Train/val Images (47 GB)</a>]


<br>
<br>

<h2>CODE</h2>
Code for training and evaluation. [<a href="https://github.com/umich-vl/relative_depth">link</a>]


<br>
<h2>Misc</h2>
This work is also featured in the first release of the <a href="https://resources.wolframcloud.com/NeuralNetRepository/">Wolfram Neural Net Repository</a>. See <a href="http://blog.wolfram.com/2018/06/14/launching-the-wolfram-neural-net-repository/">this article</a> for more details.


<br>
<br>
<tr><td align="left">
<h3>CONTACT</h3>

Please send any questions or comments to Weifeng Chen at <a href="mailto:wfchen@umich.edu">wfchen@umich.edu</a>.

</table>
</center>



</BODY>
